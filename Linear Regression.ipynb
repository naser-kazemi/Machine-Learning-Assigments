{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16adc323",
   "metadata": {},
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Linear Regression\n",
    "    </div>\n",
    "<br><br>\n",
    "<font size=4>\n",
    "   **Name**: Naser Kazemi<br>\n",
    "   **Student ID**: 99102059 <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585264a",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 1: Linear Regression Model (40 + 30 optional points)\n",
    "According to <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Classical_Models/Linear%20regression.ipynb\"><font face=\"Roboto\">Linear Regression Notebook</font></a>, train a linear regression model on an arbitrary dataset. Explain your chosen dataset and split your data into train and test sets, then predict values for the test set using your trained model. Try to find the best hyperparameters for your model. (Using Lasso Regression, Ridge Regression or Elastic Net and comparing them will have extra optional points)\n",
    "<br> Explain each step of your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1deca49",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6f7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d3a07",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Dataset we use here is the insurance dataset. This dataset contains information about the insurance charges of 1338 people. The dataset contains 7 columns, which are: age, sex, bmi, children, smoker, region and charges. The target variable is charges, which is the amount of money the insurance company charges for the person. The other columns are the features of the dataset. The dataset is available in the following link: https://www.kaggle.com/mirichoi0218/insurance\n",
    "\n",
    "# Encoding\n",
    "\n",
    "This dataset contains categorical data, so we need to encode them. We use one hot encoding for the categorical data. We use the get_dummies function from pandas to encode the data.\n",
    "\n",
    "# Splitting the data\n",
    "\n",
    "We split the data into train and test sets. We use the train_test_split function from sklearn to split the data. We use 20% of the data for testing and 80% for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698cd84",
   "metadata": {},
   "source": [
    "## Load Dataset and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaa194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1338 non-null   int64  \n",
      " 1   sex       1338 non-null   object \n",
      " 2   bmi       1338 non-null   float64\n",
      " 3   children  1338 non-null   int64  \n",
      " 4   smoker    1338 non-null   object \n",
      " 5   region    1338 non-null   object \n",
      " 6   charges   1338 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 73.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('insurance.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b3867d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "      <td>1338.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.207025</td>\n",
       "      <td>30.663397</td>\n",
       "      <td>1.094918</td>\n",
       "      <td>13270.422265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.049960</td>\n",
       "      <td>6.098187</td>\n",
       "      <td>1.205493</td>\n",
       "      <td>12110.011237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1121.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.296250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4740.287150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9382.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>34.693750</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16639.912515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.130000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>63770.428010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          bmi     children       charges\n",
       "count  1338.000000  1338.000000  1338.000000   1338.000000\n",
       "mean     39.207025    30.663397     1.094918  13270.422265\n",
       "std      14.049960     6.098187     1.205493  12110.011237\n",
       "min      18.000000    15.960000     0.000000   1121.873900\n",
       "25%      27.000000    26.296250     0.000000   4740.287150\n",
       "50%      39.000000    30.400000     1.000000   9382.033000\n",
       "75%      51.000000    34.693750     2.000000  16639.912515\n",
       "max      64.000000    53.130000     5.000000  63770.428010"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bcdfe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484db2a",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "We normalize the data to make the training process faster and more stable. We Normalize the data using the following formula:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $x$ is the data, $\\mu$ is the mean of the data, and $\\sigma$ is the standard deviation of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80f9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'smoker', 'region']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, dtype=np.int8)\n",
    "data_encoded = data_encoded.rename(columns={\"sex_female\": \"sex\", \"smoker_yes\": \"smoker\"})\\\n",
    "                .drop(columns=[\"sex_male\", \"smoker_no\"])\n",
    "\n",
    "X = data_encoded.drop(\"charges\", axis=1)\n",
    "y = data_encoded['charges']\n",
    "\n",
    "X_norm = X.copy()\n",
    "\n",
    "X_norm[['age', 'bmi']] = (X[['age', 'bmi']] - X[['age', 'bmi']].mean()) / X[['age', 'bmi']].std()\n",
    "y_norm = (y - y.mean()) / y.std()\n",
    "X_norm = X_norm.to_numpy()\n",
    "y_norm = y_norm.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317e402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8ee6e",
   "metadata": {},
   "source": [
    "# Linear Function\n",
    "\n",
    "In linear regression, we have a linear function that we want to find its parameters. The linear function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "\n",
    "where $w_0$ is the bias, $w_1$ to $w_n$ are the weights, and $x_1$ to $x_n$ are the features.\n",
    "\n",
    "# Loss Function\n",
    "\n",
    "In linear regression, we have a loss function that we want to minimize. The loss function is defined as follows:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples, $y_i$ is the true value of the $i$ th sample, and $\\hat{y}_i$ is the predicted value of the $i$ th sample. This loss function is called the Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff705456",
   "metadata": {},
   "source": [
    "# Linear Regression Using Normal Equation\n",
    "\n",
    "The linear function can be rewritten as follows: \n",
    "\n",
    "$$\n",
    "\\hat{y} = Xw\n",
    "$$\n",
    "\n",
    "where $X$ is the feature with a column of ones added to it, and $w$ is the weight vector with the bias added to it.\n",
    "\n",
    "The loss function can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{2N}(y - Xw)^T(y - Xw)\n",
    "$$\n",
    "\n",
    "where $y$ is the true value vector.\n",
    "\n",
    "The gradient of the loss function with respect to the weight vector can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}X^T(Xw - y)\n",
    "$$\n",
    "\n",
    "By setting the gradient to zero, we can find the weight vector that minimizes the loss function, we obtain:\n",
    "\n",
    "$$\n",
    "w = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "\n",
    "where $w$ is the vector of weights, $X$ is the matrix of features, and $y$ is the vector of true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96234ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0405b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30160798394871147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal equation\n",
    "w = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "\n",
    "y_pred = X_test @ w\n",
    "((y_pred - y_test) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d5a3f",
   "metadata": {},
   "source": [
    "# Linear Regression using Gradient Descent\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In linear regression using gradient descent algorithm, we want to use to find the best parameters in an iterative manner. The algorithm is defined as follows:\n",
    "\n",
    "1. Initialize the weight vector with random values.\n",
    "2. Calculate the gradient of the loss function with respect to the weight vector.\n",
    "3. Update the weight vector using the gradient by update rule:\n",
    "$$\n",
    "w_{n + 1} := w_n - \\alpha \\frac{\\partial L(w_n)}{\\partial w}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Where $L$ is the loss function, $w$ is the weight vector, and $\\alpha$ is the learning rate.\n",
    "\n",
    "4. Repeat steps 2 and 3 until the loss function converges or the maximum number of iterations is reached.\n",
    "\n",
    "in step 2, we calculate the gradient of the loss function with respect to the weight vector as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}X^T(Xw - y)\n",
    "$$\n",
    "\n",
    "In gradient descent algorithm, we have two hyper-parameters: learning rate and maximum number of iterations. The learning rate determines how much we update the weight vector in each iteration. If the learning rate is too small, the algorithm will take a long time to converge. If the learning rate is too large, the algorithm may not converge. The maximum number of iterations determines the maximum number of iterations the algorithm runs. If the algorithm does not converge in the maximum number of iterations, the algorithm stops.\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "\n",
    "In K-Fold Cross Validation, we split the data into K folds. Then, we train the model on K-1 folds and test it on the remaining fold. We repeat this process K times, each time using a different fold for testing. Finally, we average the results of the K tests to get the final result.\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "Grid search is a method to find the best hyper-parameters for a model. In grid search, we define a set of hyper-parameters and the algorithm tries all the combinations of the hyper-parameters to find the best combination. The best combination is the combination that gives the best performance on the validation set. The performance is measured by a metric, which is usually the accuracy or the loss function. Here we use the loss function as the metric and K-fold cross validation as the evaluation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e47f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, **kwargs):\n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    max_iter = kwargs.get('max_iter', 1000)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    w = np.array(np.random.rand(X.shape[1]), dtype=np.float64)\n",
    "    for i in range(max_iter):\n",
    "        y_pred = X.dot(w)\n",
    "        gradient = 1/n * X.T.dot(y_pred - y)\n",
    "        w = w - learning_rate * gradient\n",
    "    return w\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(X, y, method, k=5, **kwargs):\n",
    "    n = X.shape[0]\n",
    "    indices = np.random.permutation(n) - 1\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    fold_size = n // k\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        X_test = X[i*fold_size:(i+1)*fold_size]\n",
    "        y_test = y[i*fold_size:(i+1)*fold_size]\n",
    "        X_train = np.r_[X[:i*fold_size], X[(i+1)*fold_size:]]\n",
    "        y_train = np.r_[y[:i*fold_size], y[(i+1)*fold_size:]]\n",
    "        w = method(X_train, y_train, **kwargs)\n",
    "        y_pred = X_test @ w\n",
    "        scores.append(np.mean((y_pred - y_test)**2))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee6d1b",
   "metadata": {},
   "source": [
    "# Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698435bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/1gdchc0s1zv4lr78srz6vdd80000gn/T/ipykernel_38509/2530167079.py:28: RuntimeWarning: overflow encountered in square\n",
      "  scores.append(np.mean((y_pred - y_test)**2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0.01, 100): 0.683226429062544,\n",
       " (0.01, 500): 0.36999569776430696,\n",
       " (0.01, 1000): 0.28027812642447963,\n",
       " (0.01, 5000): 0.25936297962412946,\n",
       " (0.01, 10000): 0.2585200796308765,\n",
       " (0.05, 100): 0.3388422385060749,\n",
       " (0.05, 500): 0.2598228267206427,\n",
       " (0.05, 1000): 0.26097346262853616,\n",
       " (0.05, 5000): 0.25915736662152733,\n",
       " (0.05, 10000): 0.2631444402765875,\n",
       " (0.1, 100): 0.2870741119445859,\n",
       " (0.1, 500): 0.25917568282808984,\n",
       " (0.1, 1000): 0.25984639798586945,\n",
       " (0.1, 5000): 0.2619766650701677,\n",
       " (0.1, 10000): 0.2597265341233589,\n",
       " (0.5, 100): 0.25899457578442053,\n",
       " (0.5, 500): 0.25941834007738507,\n",
       " (0.5, 1000): 0.26039241742085034,\n",
       " (0.5, 5000): 0.2601787604944053,\n",
       " (0.5, 10000): 0.2607197497088648,\n",
       " (1, 100): 2.6074608896159157e+86,\n",
       " (1, 500): inf,\n",
       " (1, 1000): nan,\n",
       " (1, 5000): nan,\n",
       " (1, 10000): nan}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and evaluate\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = [100, 500, 1000, 5000, 10000]\n",
    "\n",
    "scores = {}\n",
    "for learning_rate in learning_rates:\n",
    "    for max_iter in max_iters:\n",
    "        score = k_fold_cross_validation(X_train, y_train, gradient_descent, learning_rate=learning_rate, max_iter=max_iter)\n",
    "        scores[(learning_rate, max_iter)] = score\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee272b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.01, 10000), 0.2585200796308765)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_score,  = min(scores.items(), key=lambda x: x[1])\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b6ab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22247973395103585,\n",
       " array([-1.01034545,  0.293039  ,  0.17965188,  0.06032784,  0.03913548,\n",
       "         2.04812882,  0.59708199,  0.56121591,  0.38134379,  0.45198632]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = gradient_descent(X_test, y_test, learning_rate=best_params[0], max_iter=best_params[1])\n",
    "y_pred = X_test @ w\n",
    "((y_pred - y_test) ** 2).mean(), w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604d0a2",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Lasso regression is a linear regression model that uses L1 regularization. Lasso regression adds the following term to the loss function:\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{N}\\sum_{i=1}^{n}|w_i|\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter. The regularization parameter determines how much we penalize the weights. If the regularization parameter is zero, the loss function is the same as the loss function of linear regression. If the regularization parameter is large, the residual sum of squares in linear regression decreases,variance of model decreases and bias increases.\n",
    "\n",
    "\n",
    "The gradient of lasso cost function with respect to the weight vector can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}(X^T(Xw - y) + \\lambda sign(w))\n",
    "$$\n",
    "\n",
    "where $sign(w)$ is the sign function that returns 1 if $w$ is positive, -1 if $w$ is negative, and 0 if $w$ is zero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fb267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(X, y, **kwargs):\n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    max_iter = kwargs.get('max_iter', 1000)\n",
    "    lambda_ = kwargs.get('lambda_', 0.1)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    w = np.array(np.random.rand(X.shape[1]), dtype=np.float64)\n",
    "    for i in range(max_iter):\n",
    "        y_pred = X.dot(w)\n",
    "        gradient = 1/n * (X.T.dot(y_pred - y) + lambda_ * np.sign(w))\n",
    "        w = w - learning_rate * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05703f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/1gdchc0s1zv4lr78srz6vdd80000gn/T/ipykernel_38509/2530167079.py:28: RuntimeWarning: overflow encountered in square\n",
      "  scores.append(np.mean((y_pred - y_test)**2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.5, 100, 1), 0.257177751606546)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = [100, 500, 1000, 5000, 10000]\n",
    "lambdas_ = [0.1, 0.5, 1, 5, 10]\n",
    "scores = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for max_iter in max_iters:\n",
    "        for lambda_ in lambdas_:\n",
    "            score = k_fold_cross_validation(X_train, y_train, lasso_regression,\n",
    "                                            learning_rate=learning_rate, max_iter=max_iter, lambda_=lambda_)\n",
    "            scores[(learning_rate, max_iter, lambda_)] = score\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "best_params, best_score = min(scores.items(), key=lambda x: x[1])\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0e67ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22299707373622624,\n",
       " array([-5.27077179e-01,  2.92122473e-01,  1.70156103e-01,  5.51552476e-02,\n",
       "         1.75341880e-02,  2.01487129e+00,  1.24237683e-01,  8.92614388e-02,\n",
       "        -5.54295740e-02, -4.39110828e-05]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = lasso_regression(X_test, y_test, learning_rate=best_params[0], max_iter=best_params[1], lambda_=best_params[2])\n",
    "y_pred = X_test @ w\n",
    "((y_pred - y_test) ** 2).mean(), w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72791a9e",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge regression is a linear regression model that uses L2 regularization. Ridge regression adds the following term to the loss function:\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{N}\\sum_{i=1}^{n}w_i^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter. The regularization parameter determines how much we penalize the weights. If the regularization parameter is zero, the loss function is the same as the loss function of linear regression. If the regularization parameter is large, the residual sum of squares in linear regression decreases,variance of model decreases and bias increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9150dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, **kwargs):\n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    max_iter = kwargs.get('max_iter', 1000)\n",
    "    lambda_ = kwargs.get('lambda_', 0.1)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    w = np.array(np.random.rand(X.shape[1]), dtype=np.float64)\n",
    "    for i in range(max_iter):\n",
    "        y_pred = X.dot(w)\n",
    "        gradient = 1/n * (X.T.dot(y_pred - y) + 2 * lambda_ * w)\n",
    "        w = w - learning_rate * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f3f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/1gdchc0s1zv4lr78srz6vdd80000gn/T/ipykernel_38509/2530167079.py:28: RuntimeWarning: overflow encountered in square\n",
      "  scores.append(np.mean((y_pred - y_test)**2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.1, 500, 0.1), 0.25737993166713674)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = [100, 500, 1000, 5000, 10000]\n",
    "lambdas_ = [0.1, 0.5, 1, 5, 10]\n",
    "scores = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for max_iter in max_iters:\n",
    "        for lambda_ in lambdas_:\n",
    "            score = k_fold_cross_validation(X_train, y_train, ridge_regression,\n",
    "                                            learning_rate=learning_rate, max_iter=max_iter, lambda_=lambda_)\n",
    "            scores[(learning_rate, max_iter, lambda_)] = score\n",
    "    \n",
    "    \n",
    "best_params, best_score = min(scores.items(), key=lambda x: x[1])\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8515c6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22250161728091114,\n",
       " array([-0.59204677,  0.29334912,  0.17917662,  0.05993538,  0.03699012,\n",
       "         2.03649347,  0.18231408,  0.14657244, -0.03133775,  0.03769973]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = ridge_regression(X_test, y_test, learning_rate=best_params[0], max_iter=best_params[1], lambda_=best_params[2])\n",
    "y_pred = X_test @ w\n",
    "((y_pred - y_test) ** 2).mean(), w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916bb38",
   "metadata": {},
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "Elastic net regression is a linear regression model that uses L1 and L2 regularization. Elastic net regression adds the following term to the loss function:\n",
    "\n",
    "$$\n",
    "\\lambda_1\\sum_{i=1}^{n}|w_i| + \\lambda_2\\sum_{i=1}^{n}w_i^2\n",
    "$$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are the regularization parameters. The regularization parameters determine how much we penalize the weights. If the regularization parameters are zero, the loss function is the same as the loss function of linear regression. If the regularization parameters are large, the residual sum of squares in linear regression decreases,variance of model decreases and bias increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f9b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic net regression\n",
    "def elastic_net_regression(X, y, **kwargs):\n",
    "    learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    max_iter = kwargs.get('max_iter', 1000)\n",
    "    lambda_1 = kwargs.get('lambda_1', 0.1)\n",
    "    lambda_2 = kwargs.get('lambda_2', 0.5)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    w = np.array(np.random.rand(X.shape[1]), dtype=np.float64)\n",
    "    for i in range(max_iter):\n",
    "        y_pred = X.dot(w)\n",
    "        gradient = 1/n * (X.T.dot(y_pred - y) + 2 * lambda_1 * w + lambda_2 * lambda_ * np.sign(w))\n",
    "        w = w - learning_rate * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faea8853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/1gdchc0s1zv4lr78srz6vdd80000gn/T/ipykernel_38509/2530167079.py:28: RuntimeWarning: overflow encountered in square\n",
      "  scores.append(np.mean((y_pred - y_test)**2))\n",
      "/var/folders/22/1gdchc0s1zv4lr78srz6vdd80000gn/T/ipykernel_38509/1753620372.py:12: RuntimeWarning: overflow encountered in add\n",
      "  gradient = 1/n * (X.T.dot(y_pred - y) + 2 * lambda_1 * w + lambda_2 * lambda_ * np.sign(w))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.1, 1000, 1, 0.1), 0.25701804632848574)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = [100, 500, 1000, 5000, 10000]\n",
    "lambdas_1 = [0.1, 0.5, 1, 5, 10]\n",
    "lambdas_2 = [0.1, 0.5, 1, 5, 10]\n",
    "scores = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for max_iter in max_iters:\n",
    "        for lambda_1 in lambdas_1:\n",
    "            for lambda_2 in lambdas_2:\n",
    "                score = k_fold_cross_validation(X_train, y_train, elastic_net_regression,\n",
    "                                                learning_rate=learning_rate, max_iter=max_iter, \n",
    "                                                lambda_1=lambda_1, lambda_2=lambda_2)\n",
    "                scores[(learning_rate, max_iter, lambda_1, lambda_2)] = score\n",
    "                \n",
    "best_params, best_score = min(scores.items(), key=lambda x: x[1])\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9a1dd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22571386866197526,\n",
       " array([-4.02316288e-01,  2.94446784e-01,  1.67984800e-01,  5.08820537e-02,\n",
       "         2.37653260e-03,  1.91863342e+00,  2.30692321e-02,  4.34366607e-04,\n",
       "        -1.39451573e-01, -8.33561109e-02]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = elastic_net_regression(X_test, y_test, learning_rate=best_params[0], max_iter=best_params[1], \n",
    "                           lambda_1=best_params[2], lambda_2=best_params[3])\n",
    "y_pred = X_test @ w\n",
    "((y_pred - y_test) ** 2).mean(), w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655c514",
   "metadata": {},
   "source": [
    "# Comparing Lasso, Ridge, and Elastic Net Regression\n",
    "\n",
    "In this section, we compare the performance of lasso, ridge, and elastic net regression on the insurance dataset. As you can see, Lasso regression has the lowest loss function, but the difference between the loss functions of lasso, ridge, and elastic net regression is not significant. Therefore, we can conclude that the performance of lasso, ridge, and elastic net regression is similar on this dataset.\n",
    "\n",
    "Lasso regression results in more sparse weight vectors than ridge regression and elastic net regression. This means that lasso regression is more likely to remove features from the model than ridge regression and elastic net regression. This is because lasso regression uses L1 regularization, which is more likely to make the weights zero than L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c685b8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have implemented linear regression using normal equation and gradient descent algorithm. We have also implemented lasso, ridge, and elastic net regression. We have used K-fold cross validation and grid search to find the best hyper-parameters for the models. We have used the Boston Housing dataset to train and evaluate the models. The results show that the models perform well on the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17834c5f8fe1aed837634498cfcf8480d7a4681659b9e2d9e3df938711b793bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
